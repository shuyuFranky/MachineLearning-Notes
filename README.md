#### 机器学习--期末复习



##### 一、监督学习

> **1. 概述**
>
> > （内容：统计学习方法，第一章；西瓜书，第二章）
> >
> > - 统计学习三要素：
> >   - 模型：模型的假设空间
> >   - 策略：模型选择的准则
> >   - 算法：模型学习的算法
> > - 损失函数和风险函数
> >
> > > - 损失函数：度量模型一次预测的好坏
> > > - 风险函数：度量平均意义下模型的好坏
> > >
> > > > 定义：损失函数的期望 （expected loss）
> > > >
> > > > 即理论上模型 $f(x)$ 关于联合分布 $P(X,Y)$ 的平均意义下的损失：$ R_{exp}(f) = E_{p}[L(Y, f(X))] = \int_{\mathcal{X}\times \mathcal{Y}}L(y, f(x))P(x,y)dxdy$
> > >
> > > **注：** 根据希望风险（$R_{exp}$）最小学习模型要用到（X,Y) 的联合分布， 而联合分布未知（若已知，则可以直接计算条件概率分布$P(Y|X)$ 即不需要学习），所以监督学习就成为一个病态问题 (ill-formed problem)。
> > >
> > > - 常用的损失函数：0-1损失函数，平方损失函数，绝对损失函数$L(Y, f(X)) = |Y - f(X)|$，对数损失函数 $L(Y, P(Y|X)) = -logP(Y|X)$
> >
> > - 经验风险 (empirical risk)
> >
> > > 定义：模型 $f(x)$ 关于训练数据集的平均损失，记作 $R_{emp}$ :
> > > $$
> > > R_{emp}(f) = \frac{1}{N}\sum_{i = 1}^{N}L(y_{i}, f(x_{i}))
> > > $$
> > > 由大数定律，当样本容量 $N$ 趋向无穷时，可以用 $R_{emp}$ 逼近 $R_{exp}$ 。
> >
> > - 监督学习的两个基本策略：
> >
> > > - 经验风险最小化
> > >
> > > $$
> > > min_{f \in \mathcal{F}}   \frac{1}{N}\sum_{i = 1}^{N}L(y_{i}, f(x_{i}))
> > > $$
> > >
> > > - 结构风险最小化：
> > >
> > >   > 在上加上了表示模型复杂程度的正则项（罚项）。
> > >
> > > $$
> > > R_{srm}(f) = \frac{1}{N}\sum_{i = 1}^{N}L(y_{i}, f(x_{i})) + \lambda J(f)
> > > $$
> > >
> >
> > - 泛化误差：
> >
> > > 学习方法对未知数据的预测能力称为泛化能力。有泛化误差上界和 Hoeffding不等式可证：训练误差小的模型，其泛化误差也会小。
> >
> > - 模型选择
> >
> > > - 正则化
> > > - 交叉验证
> >
> > - 模型概述
> >
> > > - 生成模型：朴素贝叶斯法，隐马尔可夫模型
> > > - 判别模型：k邻近法，感知机，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法，条件随机场
> >
> > - 偏差与方差分解
> >
> > > 试图对学习算法的期望泛化误差进行拆解
> > >
> > > - 偏差：$bias^{2}(x) = (\bar{f}(x) - y)^{2}$，即期望输出与真实标记的差别；
> > >
> > >
> > > - 方差：$var(x) = E_{D}[(f(x;D) - \bar{f}(x))^{2}]$ ，其中 $\bar{f} = E_{D}[f(x;D)]$ 为学习算法的期望预测，即样本数相同的不同训练集产生的方差；
> > > - 噪声：$\varepsilon ^{2} = E_{D}[(y_{D} - y)^{2}]$；
> > >
> > > 偏差、方差、噪声的含义：
> > >
> > > - **偏差：** 度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；
> > > - **方差：** 度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；
> > > - **噪声：** 在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。
> > >
> > > 于是，有
> > > $$
> > > E(f;D) = bias^{2}(x) + var(x) + \varepsilon^{2}
> > > $$
> > > 也就是说，泛化误差可以分解为偏差、方差与噪声之和。
> > >
> > > **注：偏差-方差窘境：** 
> > >
> > > > 训练不足时，训练数据的扰动不足以使学习器发生显著变化，偏差主导泛化误差；训练逐渐充足后，数据的扰动逐渐能被学习器学到，方差逐渐主导泛化误差。过拟合，即数据自生的非全局特性被学习器学到，数据发生轻微扰动都会导致学习器发生显著变化。
>
> ---
>
> 
>
> **2.==支持向量机==**
>
> > （内容：统计学习方法，第七章；西瓜书，第六章）
> >
> > - 感知机
> >
> > > - 模型：根据输入实例的特征向量 $x$ 对其进行二类分类的线性分类模型。
> > >
> > > $$
> > > f(x) = sign(w\bullet x + b)
> > > $$
> > >
> > > 对应于输入空间（特征空间）中的分离超平面 $w\bullet x + b = 0$ 。
> > >
> > > - 学习策略：极小化损失函数：
> > >
> > > $$
> > > min_{w,b}L(w, b) = - \sum_{x_{i} \in M} y_{i}(w\bullet x_{i} + b)
> > > $$
> > >
> > > 损失函数对应于误分类点到分离超平面的总距离。
> > >
> > > - 学习算法：基于SGD的对损失函数的最优化算法，有原始形式和对偶形式。
> > >
> > > > 原始形式：
> > > >
> > > > ---
> > > >
> > > > > 输入：训练数据集 $T = {(x_{1}, y_{1}), (x_{2}, y_{2}), \dots , (x_{n}, y_{n})}$ ，其中 $x_{i} \in \mathcal{X} = R^{n}$ ， $y_{i} \in \mathcal{Y} = {-1, +1}$，$i = 1,2, \dots, N$ ；学习率 $\eta (0 < \eta \leq 1)$ ；
> > > > >
> > > > > 输出：$w, b$ ；感知机模型 $f(x) = sign(w\bullet x + b)$ 。
> > > > >
> > > > > (1) 选取初始值 $w_{0}, b_{0}$
> > > > >
> > > > > (2) 在训练集中选取数据 $(x_{i}, y_{i})$
> > > > >
> > > > > (3) 如果 $y_{i}(w\bullet x_{i} + b) \leq 0$
> > > > > $$
> > > > > w \gets w + \eta y_{i}x_{i}
> > > > > $$
> > > > >
> > > > > $$
> > > > > b \gets b + \eta y_{i}
> > > > > $$
> > > > >
> > > > >  (4) 转至 (2)，直至训练集中没有误分类点。
> >
> > - SVM
> >
> > > - 概述：
> > >
> > > > - 模型：一种二分类模型（基本模型是定义在特征空间上的间隔最大的线性分类器）
> > > >
> > > > > 间隔最大使其有别与感知机；
> > > > >
> > > > > SVM还包括核技巧，使其成为实质上的非线性分类器；
> > > >
> > > > - 策略：间隔最大化，可形式化为一个求解凸二次规划问题。
> > > > - 算法：求解凸二次规划的最优化算法。
> > >
> > > - 线性可分支持向量机
> > >
> > > > 1. 函数间隔：
> > > >
> > > > > 弱点离分离平面较远，则认为其有较高的置信度被分类正确，即可以用一个点离分离平面的距离来衡量分类的确信程度。
> > > > >
> > > > > 即可定义关于样本点的函数间隔：
> > > > > $$
> > > > > \hat{\gamma}_{i} = y_{i}(w\bullet x_{i} + b)
> > > > > $$
> > > > > 和关于训练集 $T$ 的函数间隔：
> > > > > $$
> > > > > \hat{\gamma} = min_{i=1,2,\dots ,N} \hat{\gamma}_{i}
> > > > > $$
> > > > >
> > > >
> > > > 2. 几何间隔：
> > > >
> > > > > 函数间隔中 $w,b$ 成倍变化，函数间隔变大，但对应的分离平面 $w\bullet x + b = 0$ 不变。
> > > > >
> > > > > 故考虑对 $w$ 加以约束，使 $||w|| = 1$， 使得间隔是确定的。
> > > > >
> > > > > 样本点几何间隔：
> > > > > $$
> > > > > \gamma_{i} = y_{i}(\frac{w}{||w||}\bullet x_{i} + \frac{b}{||w||})
> > > > > $$
> > > > > 关于训练集T的函数间隔：
> > > > > $$
> > > > > \gamma = min_{1,2,\dots ,N} \gamma_{i}
> > > > > $$
> > > > >
> > > >
> > > > 3. 最大间隔分离超平面
> > > >
> > > > > 考虑求一个几何间隔最大的分离超平面，可以表诉为如下的约束最优化问题：
> > > > > $$
> > > > > max_{w,b} \gamma
> > > > > $$
> > > > >
> > > > > $$
> > > > > s.t. \quad y_{i}(\frac{w}{||w||}\bullet x_{i} + \frac{b}{||w||}) \geq \gamma, \quad i = 1,2,\cdots, N
> > > > > $$
> > > > >
> > > > > 将几何间隔转变为函数间隔，得：
> > > > > $$
> > > > > max_{w,b} \frac{\hat{\gamma}}{||w||}
> > > > > $$
> > > > >
> > > > > $$
> > > > > s.t. \quad y_{i}(w\bullet x_{i} + b) \geq \hat{\gamma}, \quad i = 1,2,\cdots, N
> > > > > $$
> > > > >
> > > > > 可将 $w, b$ 按比列变化，对上述优化问题没有影响，即可取 $\hat{\gamma} = 1$ 带入，并将最大化 $\frac{1}{||w||}$ 转化为最小化 $\frac{1}{2}||w||^2$ ，于是得到线性可分支持向量机学习的最优化问题（见下述4中算法——最大间隔法）。
> > > >
> > > > 4. 线性可分支持向量机学习算法——最大间隔法
> > > >
> > > > > 输入：训练数据集 $T = {(x_{1}, y_{1}), (x_{2}, y_{2}), \dots , (x_{n}, y_{n})}$ ，其中 $x_{i} \in \mathcal{X} = R^{n}$ ， $y_{i} \in \mathcal{Y} = {-1, +1}$，$i = 1,2, \dots, N$ ；
> > > > >
> > > > > 输出：最大间隔分离超平面和分类决策函数。
> > > > >
> > > > > （1）构造并求解约束最优化问题：
> > > > > $$
> > > > > min_{w,b} \frac{1}{2}||w||^{2}
> > > > > $$
> > > > >
> > > > > $$
> > > > > s.t. \quad y_{i}(w\bullet x_{i} + b) - 1\geq 0, \quad i = 1,2,\cdots, N
> > > > > $$
> > > > >
> > > > > （2）由此得到分离超平面：
> > > > > $$
> > > > > w^{*}\bullet x + b^{*} = 0
> > > > > $$
> > > > > 分类决策函数
> > > > > $$
> > > > > f(x) = sign(w^{*}\bullet x + b^{*})
> > > > > $$
> > > > >
> > > >
> > > > 5. 支持向量
> > > >
> > > > > **支持向量：** 使约束 $y_{i}(w\bullet x_{i} + b) - 1 = 0$ 等号成立的点。
> > > >
> > > > 6. 通过拉格朗日对偶性构造4中优化问题的对偶问题
> > > >
> > > > > 线性可分支持向量机学习算法
> > > > >
> > > > > > 输入：训练数据集 $T = {(x_{1}, y_{1}), (x_{2}, y_{2}), \dots , (x_{n}, y_{n})}$ ，其中 $x_{i} \in \mathcal{X} = R^{n}$ ， $y_{i} \in \mathcal{Y} = {-1, +1}$，$i = 1,2, \dots, N$ ；
> > > > > >
> > > > > > 输出：最大间隔分离超平面和分类决策函数。
> > > > > >
> > > > > > （1）构造并求解约束最优化问题：
> > > > > > $$
> > > > > > min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\bullet x_{j}) - \sum_{i=1}^{N}\alpha_{i}
> > > > > > $$
> > > > > >
> > > > > > $$
> > > > > > s.t. \quad \sum_{i=1}^{N}\alpha_{i}y_{i} = 0 \\
> > > > > > \alpha_{i} \geq 0, i = 1,2,\cdots , N
> > > > > > $$
> > > > > >
> > > > > > 求得最优解$\alpha^{*} = (\alpha_{1}^{*},\alpha_{2}^{*},\dots \alpha_{N}^{*})^{T}$ 。
> > > > > >
> > > > > > （2）计算
> > > > > > $$
> > > > > > w^{*} = \sum_{i=1}^{N}\alpha_{i}^{*}y_{i}x_{i}
> > > > > > $$
> > > > > > 并选择 $\alpha^{*}$ 的一个正分量 $\alpha_{j}^{*} > 0$ 计算
> > > > > > $$
> > > > > > b^{*} = y_{j} - \sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x_{i}\bullet x_{j})
> > > > > > $$
> > > > > > （3）求得分离超平面
> > > > > > $$
> > > > > > w^{*}\bullet x + b^{*} = 0
> > > > > > $$
> > > > > > 分类决策函数：
> > > > > > $$
> > > > > > f(x) = sign(w^{*}\bullet x + b^{*})
> > > > > > $$
> > > > > >
> > >
> > > - 线性支持向量机
> > >
> > > > 数据近似线性可分，某些样本点不能满足函数间隔大于1的约束，于是引进松弛变量 $\xi_{i} \geq 0$ ，使函数间隔加上松弛变量大于等于1.
> > > >
> > > > 1.  线性不可分的线性支持向量机的学习问题变成：
> > > >
> > > > > $$
> > > > > min_{w,b,\xi} \frac{1}{2}||w||^{2} + C\sum_{i=1}^{N}\xi_{i}
> > > > > $$
> > > > >
> > > > > $$
> > > > > s.t. \quad y_{i}(w\bullet x_{i} + b) \geq 1 - \xi_{i}, \quad i = 1,2,\cdots, N
> > > > > $$
> > > > >
> > > > > $$
> > > > > \xi_{i} \geq 0
> > > > > $$
> > > > >
> > > >
> > > > 2. 线性支持向量机学习算法
> > > >
> > > > > 输入：训练数据集 $T = {(x_{1}, y_{1}), (x_{2}, y_{2}), \dots , (x_{n}, y_{n})}$ ，其中 $x_{i} \in \mathcal{X} = R^{n}$ ， $y_{i} \in \mathcal{Y} = {-1, +1}$，$i = 1,2, \dots, N$ ；
> > > > >
> > > > > 输出：分离超平面和分类决策函数。
> > > > >
> > > > > （1）选择惩罚参数 $C > 0$ ，构造并求解凸二次规划问题
> > > > >
> > > > > （2）计算
> > > > > $$
> > > > > w^{*} = \sum_{i=1}^{N}\alpha_{i}^{*}y_{i}x_{i}
> > > > > $$
> > > > > **并选择 $\alpha^{*}$ 的一个正分量 $0 < \alpha_{j}^{*} < C$ 计算**
> > > > > $$
> > > > > b^{*} = y_{j} - \sum_{i=1}^{N}\alpha_{i}^{*}y_{i}(x_{i}\bullet x_{j})
> > > > > $$
> > > > > （3）求得分离超平面
> > > > > $$
> > > > > w^{*}\bullet x + b^{*} = 0
> > > > > $$
> > > > > 分类决策函数：
> > > > > $$
> > > > > f(x) = sign(w^{*}\bullet x + b^{*})
> > > > > $$
> > > > > **注：** 对任一适合条件 $0 < \alpha_{j}^{*} < C$ 的$\alpha^{*}$，都可以求出 $b^{*}$ ，实际计算时可以取所有符合条件的平均值。
> > >
> > > - 非线性支持向量机
> > >
> > > > 当输入空间为 **欧式空间** 或 **离散集合** 、特征空间为 **希尔伯特** 空间时，核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。
> > > >
> > > > 通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维空间中学习线性支持向量机。这样的方法称为核技巧。核方法是比支持向量机更为一般的机器学习方法。
>
> ---
>
> 
>
> 3. **KNN**
>
> > （内容：统计学习方法，第三章；西瓜书，第十章第1节）
> >
> > 算法简介：根据给定的距离度量方法，获得输入样本 $x$ 最近的 $k$ 个点，通过投票表决等分类决策规则决定 $x$ 的类别 $y$ 。k-NN 算法没有显示的学习过程，是一种 Lazy 的学习模型。
> >
> > - 三要素
> >
> > > 1. 距离度量
> > > 2. $k$ 值的选择
> > > 3. 分类决策规则决定
>
> ---
>
> 
>
> 4. **朴素贝叶斯法**
>
> > （内容：统计学习方法，第四章；西瓜书，第七章；）
> >
> > 对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。
> >
> > > 1. 样本条件风险（将输入 $x$ 判断为 $c_{i}$ 类的期望损失）：
> > >
> > > $$
> > > R(c_{i}|x) = \sum_{j=1}^{N}\lambda_{ij}P(c_{j}|x)
> > > $$
> > >
> > > > 其中 $\lambda_{ij}$ 表示将真实标签为 $j$ 的输入 $x$ 判定为 $i$ 的损失。
> > > >
> > > > 即，分类任务是要找一个判定准则 $h: \mathcal{X} \mapsto \mathcal{Y}$ 以最小化总体风险
> > >
> > > $$
> > > R(h) = E_{x}[R(h(x)|x)]
> > > $$
> > >
> > > 2. 贝叶斯判定准则：
> > >
> > > > 为最小化总体风险，只需要在每个样本上选择那个能使条件风险 $R(c|x)$ 最小的类别标记，即
> > >
> > > $$
> > > h^{*}(x) = arg min_{c \in \mathcal{Y}}R(c|x)
> > > $$
> > >
> > > > 此时，$h^{*}$称为贝叶斯最优分类器，与之对应的总体风险 $R(h^{*})$ 称为贝叶斯风险。$1-R(h^{*})$ 反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。
> > >
> > > 3. 若定义 $\lambda_{ij}$ 为 0-1 损失函数，
> > >
> > > > $$
> > > > \lambda_{ij} = 0, \quad if \quad i = j; \\
> > > > =1, otherwise
> > > > $$
> > > >
> > > > 则 ，$R(c_{i}|x) = 1 - P(c|x)$ ，即要最大化 $P(c|x)$ ，最小化分类错误率的贝叶斯最优分类器为
> > > > $$
> > > > h^{*} = arg max_{c \in  \mathcal{Y}}P(c|x)
> > > > $$
> > > > 即对每个样本 $x$ ，选择能使后验概率 $P(c|x)$ 最大的类别标记。
> > > >
> > > > 要求 $P(c|x)$ 通常难以直接获得，两种策略：
> > > >
> > > > > 1. 直接建模 $P(c|x)$ 来预测 $c$ ，这样得到的是 **判别模型** ；
> > > > > 2. 先对联合分布 $P(x, c)$ 建模，然后计算 $P(c|x)$ ，这样得到的是 **生成模型** 。
> > > >
> > > > 如2中所述，对于生成模型：
> > > > $$
> > > > P(c|x) = \frac{P(x,c)}{P(x)}
> > > > $$
> > > > 由贝叶斯定理得：
> > > > $$
> > > > P(c|x) = \frac{P(c)P(x|c)}{P(x)}
> > > > $$
> > > > 其中，$P(x)$ 与类标记 $c$ 无关。
> > >
> > > 4. 朴素贝叶斯分类器
> > >
> > > > 3中计算后验概率 $P(c|x)$ 的主要困难在于计算 $P(x|c)$ ，类条件概率是所有属性上的联合概率分布。
> > > >
> > > > 为避开这一困难 **朴素** 贝叶斯即假设 **所有属性相互独立** ，即每个属性独立地对分类结果发生影响。
> > > >
> > > > - 于是得贝叶斯判定准则有，即朴素贝叶斯分类器：
> > > >
> > > > $$
> > > > h^{*} = arg max_{c \in  \mathcal{Y}}P(c|x) = argmax_{c \in \mathcal(Y)} P(c)P(x|c) \\ 
> > > > = argmax_{c\in \mathcal{Y}}P(c)\prod_{i=1}^{d}P(x_{i}|c)
> > > > $$
> > > >
>
> ---
>
> 
>
> 5. **决策树模型**
>
> > （内容：统计学习方法，第五章；西瓜书，第四章）
> >
> > 1. 信息增益
> >
> > > - 熵：随机变量不确定性的度量，$H(X)$
> > >
> > > $$
> > > P(X = x_{i}) = p_{i}, \quad i = 1, 2, \dots ,n \\
> > > H(X) = -\sum_{i=1}^{n}p_{i}log(p_{i})
> > > $$
> > >
> > > - 条件熵：已知随机变量 $X$ 的情况下，随机变量 $Y$ 的不确定性，$H(Y|X)$
> > >
> > > $$
> > > P(X = x_{i}, Y = y_{i}) = p_{ij}, \quad i = 1, 2, \dots , m \\
> > > H(Y|X) = \sum_{i = 1}^{n}p_{i}H(Y|X = x_{i})
> > > $$
> > >
> > > 这里 $p_{i} = P(X = x_{i})$。
> > >
> > > **注：** 当熵和条件熵中的概率由数据估计（极大似然估计）得到时，所对应的熵与条件熵分别称为 **经验熵** 和 **经验条件熵** 。
> > >
> > > - 信息增益：得知特征 $X$ 的信息而是得类 $Y$ 的不确定性减少的程度。
> > >
> > > $$
> > > g(D, A) = H(D) - H(D|A)
> > > $$
> > >
> >
> > 2. 信息增益算法
> >
> > > 输入：训练数据集 $D$ 和特征 $A$ ；
> > >
> > > 输出：特征 $A$ 对训练集 $D$ 的信息增益。
> > >
> > > （1）计算数据集 $D$ 的经验熵 $H(D)$
> > > $$
> > > H(D) = -\sum_{k = 1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}
> > > $$
> > > （2）计算特征 $A$ 对数据集 $D$ 的经验条件熵 $H(D|A)$ 
> > > $$
> > > H(D|A) = \sum_{i = 1}^{n}\frac{|D_{i}|}{D}H(D_{i}) \\
> > > = \sum_{i = 1}^{n}\frac{|D_{i}|}{|D|}\sum_{k = 1}^{K}\frac{|D_{ik}|}{|D_{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}
> > > $$
> > > （3）计算信息增益
> > > $$
> > > g(D, A) = H(D) - H(D|A)
> > > $$
> > >
> >
> > 3. 信息增益比
> >
> > > **信息增益** 作为划分训练数据集的特征，存在 **偏向** 于选择取值较多的特征的问题。使用 **信息增益比** 可以对这一问题进行矫正。
> > >
> > > 定义信息增益比 $g_{R}(D, A)$
> > > $$
> > > g_{R}(D, A) = \frac{g(D, A)}{H_{A}(D)}
> > > $$
> > > 其中，$H_{A}(D) = -\sum_{i = 1}^{n}\frac{|D_{i}|}{|D|}log_{2}\frac{|D_{i}|}{|D|}$ ，$n$ 是特征 $A$ 取值个数。
> >
> > 3. ID3 算法
> >
> > > **核心：** 在每个节点上用 **信息增益** 准则选择特征，递归地构建决策树。
> > >
> > > 输入：训练数据集 $D$ ，特征集 $A$ ，阈值 $\varepsilon$；
> > >
> > > 输出：决策树 $T$ 。
> > >
> > > （1）若 $D$ 中所有实例属于同一类 $C_{k}$ ，则 $T$ 为单节点树，并将 $C_{k}$ 作为该节点的类标记，返回 $T$ ；
> > >
> > > （2）若 $A = \emptyset$ ， 则 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该节点的类标记，返回 $T$ ；
> > >
> > > （3）否则，计算 $A$ 中各个特征对 $D$ 的 **信息增益** ，选择 **信息增益** 最大的特征 $A_{g}$ ；
> > >
> > > （4）如果 $A_{g}$ 的信息增益小于阈值 $\varepsilon$ ，则置 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该节点的类标记，返回 $T$ ;
> > >
> > > （5）否则，对 $A_{g}$ 的每一个可能值 $a_{i}$ ，依 $A_{g} = a_{i}$ 将 $D_{i}$ 中实例数最大的类作为标记，构建子节点，由fang'hui返回节点及其子节点构成树 $T$，返回 $T$ ；
> > >
> > > （6）对第 $i$ 个子节点，以 $D_{i}$ 为训练集，以 $A - {A_{g}}$ 为特征集，递归地调用步 (1) ~ (5)，得到子树 $T_{i}$ ，返回 $T_{i}$ 。
> >
> > 4. C4.5 算法
> >
> > > C4.5 算法与 ID3 算法相似，是对 ID3 算法的改进。
> > >
> > > **核心：** 用 **信息增益比** 来选择特征。
> > >
> > > 输入：训练数据集 $D$ ，特征集 $A$ ，阈值 $\varepsilon$；
> > >
> > > 输出：决策树 $T$ 。
> > >
> > > （1）若 $D$ 中所有实例属于同一类 $C_{k}$ ，则 $T$ 为单节点树，并将 $C_{k}$ 作为该节点的类标记，返回 $T$ ；
> > >
> > > （2）若 $A = \emptyset$ ， 则 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该节点的类标记，返回 $T$ ；
> > >
> > > （3）否则，计算 $A$ 中各个特征对 $D$ 的 **信息增益比** ，选择 **信息增益比** 最大的特征 $A_{g}$ ；
> > >
> > > （4）如果 $A_{g}$ 的信息增益比小于阈值 $\varepsilon$ ，则置 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_{k}$ 作为该节点的类标记，返回 $T$ ;
> > >
> > > （5）否则，对 $A_{g}$ 的每一个可能值 $a_{i}$ ，依 $A_{g} = a_{i}$ 将 $D_{i}$ 中实例数最大的类作为标记，构建子节点，由fang'hui返回节点及其子节点构成树 $T$，返回 $T$ ；
> > >
> > > （6）对第 $i$ 个子节点，以 $D_{i}$ 为训练集，以 $A - \{A_{g}\}$ 为特征集，递归地调用步 (1) ~ (5)，得到子树 $T_{i}$ ，返回 $T_{i}$ 。
> >
> > 5. CART 算法
> >
> > > Classification and regression tree, CART
> > >
> > > （1）决策树的生成：基于训练数据生成尽可能大的二叉决策树；
> > >
> > > （2）决策树的剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝标准。
> > >
> > > - 回归树生成
> > >
> > > > **最小二乘回归树生成算法**
> > > >
> > > > 输入：训练数据集 $D$ ；
> > > >
> > > > 输出：回归树 $f(x)$ 。
> > > >
> > > > 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：
> > > >
> > > > （1）选择最优切分变量 $j$ 与切分点 $s$ ，求解
> > > > $$
> > > > min_{j,s}[min_{c_{1}}\sum_{x_{i} \in R_{1}(j, s)}(y_{i} - c_{1})^{2} + min_{c_{2}}\sum_{x_{i} \in R_{2}(j, s)}(y_{i} - c_{2})^{2}]
> > > > $$
> > > > 遍历变量 $j$ ，对固定的切分点 $s$ ，选择使上式达到最小值的对 $(j, s)$。
> > > >
> > > > （2）用选定的 $(j, s)$ 划分区域并决定相应的输出值：
> > > > $$
> > > > R_{1}(j, s) = \{x| x^{j} \leq s\}, \quad R_{2}(j, s) = \{x|x^{j} > s \}
> > > > $$
> > > >
> > > > $$
> > > > \hat{c} = \frac{1}{N_{m}}\sum_{x_{i} \in R_{m}(j, s)}y_{i}, \quad x \in R_{m}, \quad m = 1,2
> > > > $$
> > > >
> > > > （3）继续对两个子区域调用步骤（1）,（2），直至满足停止条件。
> > > >
> > > > （4）将输入空间划分为 $M$ 个区域 $R_{1}, R_{2}, \dots, R_{m}$ ，生成决策树：
> > > > $$
> > > > f(x) = \sum_{m = 1}^{M}\hat{c}_{m}I(x \in R_{m})
> > > > $$
> > > >
> > >
> > > - 分类树生成
> > >
> > > > - 基尼指数：分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 个类的概率为 $p_{k}$，则概率分布的基尼指数定义为
> > > >
> > > > $$
> > > > Gini(p) = \sum_{k = 1}^{K}p_{k}(1 - p_{k}) = 1 - \sum_{k = 1}^{K}p_{k}^{2}
> > > > $$
> > > >
> > > > 对于二分类问题，若样本点属于第1个类的概率是 $p$ ，则
> > > > $$
> > > > Gini(p) = 2p(1-p)
> > > > $$
> > > > 对于给定的样本集合 $D$，其基尼指数为
> > > > $$
> > > > Gini(D) = 1 - \sum_{k = 1}^{K}(\frac{|C_{k}|}{|D|})^{2}
> > > > $$
> > > > 在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为
> > > > $$
> > > > Gini(D, A) = \frac{|D_{1}|}{|D|}Gini(D_{1}) + \frac{|D_{2}|}{|D|}Gini(D_{2})
> > > > $$
> > > >
> > > > - CART 算法
> > > >
> > > > > 输入：训练数据集 $D$ ， 停止计算的条件；
> > > > >
> > > > > 输出：CART 决策树 。
> > > > >
> > > > > 根据训练数据集，从根节点开始，递归地对每个节点进行一下操作，构建二叉决策树：
> > > > >
> > > > > （1）设节点的训练数据集 $D$ ，计算现有特征对该数据集的基尼指数。此时，对每一个特征 $A$ ，对其可能的每个取值 $a$ ，根据样本点对 $A = a$ 的测试为『是』和『否』将 $D$ 分割为 $D_{1}$ 和 $D_{2}$ 两部分，计算 $A = a$ 时的基尼指数。
> > > > >
> > > > > （2）在所有 $A$ 和切分点 $a$ 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依据最优切分点，从现节点生成两个子节点，将训练数据集依特征分配到两个子节点中去。
> > > > >
> > > > > （3）对两个子节点调用步骤（1）,（2），直至满足停止条件。
> > > > >
> > > > > （4）生成 CART 决策树。
> >
> > 6. 剪枝
> >
> > > - 树的剪枝算法：
> > >
> > > > （见《统计学习方法》P70，算法 5.6）
> > >
> > > - CART 剪枝算法：
> > >
> > > > (见《统计学习方法》P73，算法 5.7）
>
> ---
>
> 
>
> 6. 逻辑斯谛回归与最大熵模型
>
> > （内容：统计学习，第六章；🙃）
>
> ---
>
>  
>
> 7. ==提升方法==
>
> > （内容：统计学习，第八章；西瓜书，第八章）
> >
> > 重点：Adaboost、提升树
>
> ---
>
> 
>
> 8. EM 算法
>
> > （内容：统计学习，第九章；西瓜书，第七章，7.6 小节）
>
> ---
>
>  
>
> 9. 隐马尔可夫模型
>
> > （内容：统计学习，第十章；西瓜书，第十四章）



---

---





##### 二、非监督学习

> 1. 频繁模式挖掘
>
> > （内容：数据挖掘：概念与技术，第六章）
> >
> > - Apriori 算法
> > - FP-增长算法
>
> 2. 聚类
>
> > （内容：西瓜书，第九章；数据挖掘：概念与技术，第八章）
> >
> > K-means , K-medoids，学习向量化
>
> 3. 异常点挖掘
>
> > （内容：数据挖掘：概念与技术，第八章）
>
> - 增强学习
>
> > （内容：西瓜书，第十六章）
> >
> > Bellman

